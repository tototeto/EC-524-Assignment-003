---
title: '003'
author: "Tom Ben-Shahar"
date: "March 3, 2025"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{R Setup, warning=FALSE}
# Load packages
library(pacman)
p_load(palmerpenguins, tidymodels, tidyverse, magrittr, skimr, DescTools, collapse, janitor, rpart, rpart.plot, doParallel, ranger, xgboost)
library(tidymodels)
# Load the penguin data
data('penguins')

penguins %<>% clean_names()
```

# Data Cleaning

### Use skimr to check out the data. How many species do we have? Are they fairly balanced in the data? Why does it matter?

```{r DataClean1, warning=FALSE}
skim(penguins)

ggplot(penguins, aes(x = species)) +
  geom_bar(fill = "cyan4", color = "black") +
  labs(title = "Number of Penguins by Species",
       x = "Species",
       y = "Count") +
  theme_minimal()
```

Penguins species are not equally represented in the data, with some species having more than twice as many observations as others.

### What is imputation?

Imputation is the process for handling missing data in which the values missing are replaced with guesses based on similar data points.

### Hopefully you noticed we have some missing data. Let's impute. By hand. Since we're doing it by hand, let's start simply. For categorical variables, you can use the modal class. For numeric, let's use the median.

```{r HandImputation, warning=FALSE}
pen_imp_hand <- penguins %>% 
  mutate(
    # Numeric
    bill_length_mm = if_else(is.na(bill_length_mm), mean(bill_length_mm, na.rm = TRUE), bill_length_mm),
    bill_depth_mm = if_else(is.na(bill_depth_mm), mean(bill_depth_mm, na.rm = TRUE), bill_depth_mm),
    flipper_length_mm = if_else(is.na(flipper_length_mm), mean(flipper_length_mm, na.rm = TRUE), flipper_length_mm),
    body_mass_g = if_else(is.na(body_mass_g), mean(body_mass_g, na.rm = TRUE), body_mass_g),
    
    # Categorical
    sex = if_else(is.na(sex), Mode(sex, na.rm=TRUE), sex),
    species = if_else(is.na(species), Mode(species, na.rm=TRUE), species),
    island = if_else(is.na(island), Mode(island, na.rm=TRUE), island)
  )
```

### Now repeat using tidymodels. Make a recipe and then prep and bake it. Check out the objects!

```{r tidyImpute, warning=FALSE}
rec_imp <- recipe(penguins) %>% 
  step_impute_mean(all_numeric()) %>% 
  step_impute_mode(all_factor())

pen_imp_tidy <- rec_imp %>% prep() %>% juice()

rec_imp

skim(pen_imp_tidy)
```

### How could we be more sophisticated/clever about imputation? Why would it matter?

We could do something more complicated than just mean/mode. For example, we could use a method like KNN to inform the imputation based on *similar* data points, not just all the data points. This could potentially give us more accurate imputations closer to realistic values.

# Short Tree

### For the moment, let's focus on the possible splits of the island variable. There are three islands. How many possible splits are there?

Only 3, as you can separate any 1 from the other 2.

### Try each split of island and then calculate the split's (1) accuracy and (2) gini.

```{r manualSplits, warning=FALSE}
# Find the islands
islands = penguins$island |> unique()

suppress <- lapply(X = islands, FUN = function(i) {

  grp1 = penguins |> filter(island == i)
  grp2 = penguins |> filter(island != i)
  # Find the modal species in each group
  species1 = grp1$species |> fmode()
  species2 = grp2$species |> fmode()
  # Calculate accuracy
  print(paste("Accuracy, ", i, ": ", fmean(grp1$species == species1)))
  print(paste("Accuracy, ", i, ": ", fmean(grp2$species == species2)))
  # Calculate gini
  g1 = grp1$species |> table() |> prop.table()
  g2 = grp2$species |> table() |> prop.table()
  gini1 = sum(g1 * (1 - g1))
  gini2 = sum(g2 * (1 - g2))
  print(paste("Gini, ", i, ": ", (gini1 + gini2)/2))
})
```

### Repeat 1.2 but for the sex variable.

```{r giniSex, warning=FALSE}
# Find the sex
sex = penguins$sex |> unique()

suppress_2 <- lapply(X = sex, FUN = function(i) {

  grp1 = penguins |> filter(sex == i)
  grp2 = penguins |> filter(sex != i)
  # Find the modal species in each group
  species1 = grp1$species |> fmode()
  species2 = grp2$species |> fmode()
  # Calculate accuracy
  print(paste("Accuracy, ", i, ": ", fmean(grp1$species == species1)))
  print(paste("Accuracy, ", i, ": ", fmean(grp2$species == species2)))
  # Calculate gini
  g1 = grp1$species |> table() |> prop.table()
  g2 = grp2$species |> table() |> prop.table()
  gini1 = sum(g1 * (1 - g1))
  gini2 = sum(g2 * (1 - g2))
  print(paste("Gini, ", i, ": ", (gini1 + gini2)/2))
})
```

I have absolutely no idea why this is returning a third set of statistics, as there are only two levels to the variable **sex**.

### Which variable and split would you make? Why?

I would split **island** by *Biscoe* and *Not Biscoe*, as this seems to provide the best accuracy and similar Gini compared to other splits.

### We would typically want to consider all possible splits for the numeric variables as well. Explain (or code) an approach that would try all of the splits for bill_length_mm.

```{r numericSplitManual, warning=FALSE}
# Use imputed values
penguins <- pen_imp_tidy

# Define resolution
res = 1

# Define range
range = seq(from=min(penguins$bill_length_mm),to=(max(penguins$bill_length_mm)), by = res)

suppress_3 <- lapply(X = range, FUN = function(i) {
  
  p_low <- penguins %>% filter(bill_length_mm <= i)
  p_high <- penguins %>% filter(bill_length_mm > i) 
  mode_low <- Mode(as.character(p_low$species))
  mode_high <- Mode(as.character(p_high$species))
  acc_df <- penguins %>% select(bill_length_mm, species)
  acc_df %<>% mutate(
    pred = factor(ifelse(bill_length_mm <= i, mode_low, mode_high))
  ) 
  levels(acc_df$pred) <- c("Adelie", "Chinstrap",  "Gentoo")
  a = accuracy(acc_df, truth = species, estimate = pred)
  print(paste("Accuracy, ", i, ": ", a[3]))
})
```

# **Bigger** Tree

### Use tidymodels to grow a tree (actually multiple trees). Remember: We can (need to) tune the cost complexity (cost_complexity) to try to restrict the tree from overfitting.

```{r treeModel, warning=FALSE}
set.seed(981598345)
# Reload data
data('penguins')

# Define Recipe

rec_1 <- recipe(penguins, species ~ island + sex + bill_length_mm) %>% 
  step_impute_mean(all_numeric()) %>% 
  step_impute_mode(all_factor()) %>% 
  update_role(species, new_role = "outcome")

# Define Model

mod_1 <- decision_tree(
  mode = "classification",
  engine = "rpart",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
)

# Define Workflow

wkfl_1 <- workflow() %>% 
  add_model(mod_1) %>% 
  add_recipe(rec_1)

# Cross Validation sets
nfold = 5
peng_cv <- rec_1 %>% prep %>% juice %>% vfold_cv(v = nfold)

# Parameter ranges

cost_comp = 10^seq(from = -6, to = 0, by = 0.5)
depth = seq(from = 1, to = 15, by = 1)
mn = seq(from = 1, to = 10, by = 2)

# Parellel

tgrid <- expand_grid(
      cost_complexity = cost_comp,
      tree_depth = depth,
      min_n = mn)

# rf_tuning = mclapply(
#   x = 1:nrow(tgrid),
#   FUN 
# )

# cross validate

# Detect the number of available cores
num_cores <- detectCores()

# Register parallel backend
cl <- makeCluster(num_cores - 1)
registerDoParallel(cl)

cv <- wkfl_1 %>% 
  tune_grid(
    peng_cv,
    grid = tgrid,
    metrics = metric_set(accuracy),
    control = control_grid(save_pred = TRUE, verbose = TRUE, parallel_over = "everything") # Parellel
  )

show_best(cv)

fit_1 <- wkfl_1 %>% 
  finalize_workflow(select_best(cv, metric = 'accuracy')) %>% 
  fit(data = penguins)

rpart.plot(
  fit_1$fit$fit$fit,
  extra = 104,
  box.palette = "RdPu",
  branch.lty = 4,
  shadow.col = "gray",
  nn = TRUE,
  cex = 0.5
)
```

The best models seem to choose very low cost_complexity, indicating that little pruning is needed.

# More Trees

### Briefly explain how you could build upon your work in Part 1 to grow a random forest.

I could change the model from decision_tree() to rand_forest()

### Now use tidymodels to tune (with CV) a random forest. Make sure you tune the relevant parameters (what are they?).

```{r randomForest, warning=FALSE}
set.seed(35480384)
# Reload data
data('penguins')

# Define Model

mod_rf <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>% 
  set_engine("ranger")

# Define Workflow

wkfl_rf <- workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rec_1)

# Parameter ranges

mtry_r = seq(from = 1, to = ncol(penguins), length.out = ncol(penguins))
trees_r = floor(10^seq(from = 0, to = 3, by = 0.5))
min_n_r = seq(from = 1, to = 10, by = 2)

# Parellel

tgrid_rf <- expand_grid(
      mtry = mtry_r,
      trees = trees_r,
      min_n = min_n_r)

registerDoParallel(cl)

cv_rf <- wkfl_rf %>% 
  tune_grid(
    peng_cv,
    grid = tgrid_rf,
    metrics = metric_set(accuracy),
    control = control_grid(save_pred = TRUE, verbose = TRUE, parallel_over = "everything") # Parellel
  )

best_rf <- show_best(cv_rf)

fit_rf <- wkfl_rf %>% 
  finalize_workflow(select_best(cv_rf, metric = 'accuracy')) %>% 
  fit(data = penguins)

cat(
  "\n------------------------------------------------\n",
  "Tuned Parameters:\nNumber of Predictors Sampled:",
  best_rf$mtry[1] , "\nNumber of Trees: ",
  best_rf$trees[1], "\nMinimum Data Points per Node: ", best_rf$min_n[1],
  "\nAccuracy: ", best_rf$mean[1],
  "\n------------------------------------------------\n"
)

```

### Now update your workflow to use "fancier" approaches for imputation. Rerun your CV.

```{r fancyRF, warning=FALSE}
# f for fancy
rec_f <- recipe(penguins, species ~island + sex + bill_length_mm  + body_mass_g) %>% 
  step_impute_bag(all_numeric(), all_factor()) %>% 
  update_role(species, new_role = "outcome")

wkfl_f <- workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rec_f)

registerDoParallel(cl)

peng_cv_f <- rec_f %>% prep() %>% juice() %>% vfold_cv(v = nfold)

cv_f <- wkfl_f %>% 
  tune_grid(
    peng_cv_f,
    grid = tgrid_rf,
    metrics = metric_set(accuracy),
    control = control_grid(save_pred = TRUE, verbose = TRUE, parallel_over = "everything") # Parellel
  )

best <- show_best(cv_f)

# Print results

cat(
  "\n------------------------------------------------\n", 
  "Tuned Parameters:\nNumber of Predictors Sampled:",
  best$mtry[1] , "\nNumber of Trees: ",
  best$trees[1], "\nMinimum Data Points per Node: ", best$min_n[1],
  "\nAccuracy: ", best$mean[1], 
  "\n------------------------------------------------\n"
)

```

### How does this fancier imputation affect your performance?

Fancier imputation helped accuracy a bit, but this may be because I changed the recipe slightly also.

### Explain why it might be important to include imputation in cross validation (rather than finishing all of the data cleaning before you tune/train/validate).

By cross-validating the imputation method, you can systematically ensure that you use the most effective method. This is better than just guessing " maybe KNN will work"

# Boosting

### Briefly explain how boosting extends/builds upon your work in Part 1.

Boosting creates a forest of trees, but with the added benefit of allowing trees to inform one another.

### Now use tidymodels to tune (with CV) a boosted tree. Make sure you tune the relevant parameters (what are they?).

```{r boosting, warning=FALSE}

# This chunk isn't running properly, figure out tomorrow

mod_b <- boost_tree(
  mode = "classification",
  engine = "xgboost",
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  learn_rate = tune(),
  tree_depth = tune(),
)

rec_b <- recipe(penguins, species ~island + sex + bill_length_mm  + body_mass_g) %>% 
  step_impute_bag(all_numeric(), all_factor()) %>% 
  update_role(species, new_role = "outcome") %>% 
  step_dummy(all_factor_predictors())

# Define parameter ranges
mtry_b = seq(from = 1, to = ncol(penguins), length.out = ncol(penguins))
trees_b = floor(10^seq(from = 0, to = 3, by = 1))
min_n_b = seq(from = 1, to = 10, by = 5)
learn_rate_b = (10^seq(from = -4, to = 0, by = 0.5))
tree_depth_b = seq(from=1,to=10,by=2)

wkfl_b <-workflow() %>% 
  add_model(mod_b) %>% 
  add_recipe(rec_b)

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

tgrid_b <- expand_grid(
      mtry = mtry_b,
      trees = trees_b,
      min_n = min_n_b,
      learn_rate = learn_rate_b,
      tree_depth = tree_depth_b)

cv_b <- wkfl_b %>% 
  tune_grid(
    peng_cv_f,
    grid = tgrid_b,
    metrics = metric_set(accuracy),
    control = control_grid(save_pred = TRUE, verbose = TRUE, parallel_over = "everything") # Parellel
  )

stopCluster(cl)

best_b <- show_best(cv_b)

# Print results

cat(
  "\n------------------------------------------------\n", 
  "Tuned Parameters:\nNumber of Predictors Sampled:",
  best_b$mtry[1] , "\nNumber of Trees: ",
  best_b$trees[1], "\nMinimum Data Points per Node: ", best_b$min_n[1],
  "\nAccuracy: ", best_b$mean[1], 
  "\nShrinkage Parameter: ", best_b$learn_rate[1],
  "\nTree Depth: ", best_b$tree_depth[1],
  "\n------------------------------------------------\n"
)

```

### How does boosting compare to random forests?

Accuracy for both models was identical. However, random forests ran significantly faster, so in that sense there are some efficiency gains. 






























